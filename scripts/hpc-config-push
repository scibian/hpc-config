#!/usr/bin/env python3
# -*- coding: utf-8 -*-
#
# Copyright (C) 2016-2017 EDF SA
# Contact:
#       CCN - HPC <dsp-cspit-ccn-hpc@edf.fr>
#       1, Avenue du General de Gaulle
#       92140 Clamart
#
# Authors: CCN - HPC <dsp-cspit-ccn-hpc@edf.fr>
#
# This file is part of puppet-hpc.
#
# puppet-hpc is free software: you can redistribute in and/or
# modify it under the terms of the GNU General Public License,
# version 2, as published by the Free Software Foundation.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public
# License along with puppet-hpc. If not, see
# <http://www.gnu.org/licenses/>.

import os
import argparse
import configparser
from io import StringIO
import tarfile
import sys
import logging
logger = logging.getLogger(__name__)
import tempfile
import shutil
import boto
import boto.s3
import boto.s3.connection
import paramiko
import stat
import socket
import warnings
import time
import hashlib
from multiprocessing.dummy import Pool as ThreadPool
from multiprocessing import Pool

def conf_copy(src, dst, *, follow_symlinks=True):
    """Alternate copy function for shutil.copytree() in order to properly
       resolve and copy symlinks to directories. It is used to copy private
       files into the tmp directory. If the path is a directory, call copytree()
       with self as copy_function, otherwise (flat file) call copy2()."""
    if os.path.isdir(src):
        shutil.copytree(src, dst,
                        symlinks=not follow_symlinks,
                        copy_function=conf_copy)
    else:
        shutil.copy2(src, dst, follow_symlinks=follow_symlinks)

class AppConf():
    """Runtime configuration class."""

    def __init__(self):

        self.debug = False
        self.conf_file = None
        self.cluster = None
        self.environment = None
        self.version = None

        self.mode = None

        ## Common parameters
        self.destination_root = None

        ## Posix Parameters
        self.posix_file_mode = None
        self.posix_dir_mode = None

        ## S3 parameters
        self.s3_access_key = None
        self.s3_secret_key = None
        self.s3_bucket_name = None
        self.s3_host = None
        self.s3_port = None

        ## SFTP parameters
        self.sftp_hosts = None
        self.sftp_username = None
        self.sftp_private_key = None

        # action

        self.full_tmp_cleanup = False

        # paths

        self.conf_puppet = None
        self.conf_hiera = None
        # This one is hard-coded, there is no configuration parameter to
        # change it since it would be irrelevant to change it.
        self.conf_environment = 'environment.conf'
        self.facts_private = None
        self.dir_modules_generic = None
        self.dir_modules_private = None
        self.dir_manifests_generic = None
        self.dir_manifests_private = None
        self.dir_hieradata_generic = None
        self.dir_hieradata_private = None
        self.dir_files_private = None


        self.dir_tmp = None
        self.dir_tmp_gen = None

    def dump(self):
        logger.debug("runtime configuration dump:")
        logger.debug("- debug: %s", str(self.debug))
        logger.debug("- conf_file: %s", str(self.conf_file))
        logger.debug("- cluster: %s", str(self.cluster))
        logger.debug("- environment: %s", str(self.environment))
        logger.debug("- version: %s", str(self.version))
        logger.debug("- mode: %s", str(self.mode))
        logger.debug("- destination_root: %s", str(self.destination_root))
        logger.debug("- destination: %s", str(self.destination))
        logger.debug("- dir_tmp: %s", str(self.dir_tmp))
        logger.debug("- conf_puppet: %s", str(self.conf_puppet))
        logger.debug("- conf_hiera: %s", str(self.conf_hiera))
        logger.debug("- facts_private: %s", str(self.facts_private))
        logger.debug("- dir_modules_generic: %s", str(self.dir_modules_generic))
        logger.debug("- dir_modules_private: %s", str(self.dir_modules_private))
        logger.debug("- dir_manifests_generic: %s", str(self.dir_manifests_generic))
        logger.debug("- dir_manifests_private: %s", str(self.dir_manifests_private))
        logger.debug("- dir_hieradata_generic: %s", str(self.dir_hieradata_generic))
        logger.debug("- dir_hieradata_private: %s", str(self.dir_hieradata_private))
        logger.debug("- dir_files_private: %s", str(self.dir_files_private))
        logger.debug("- posix_file_mode: %s", str(self.posix_file_mode))
        logger.debug("- posix_dir_mode: %s", str(self.posix_dir_mode))
        logger.debug("- s3_access_key: %s", str(self.s3_access_key))
        logger.debug("- s3_secret_key: %s", str(self.s3_secret_key))
        logger.debug("- s3_bucket_name: %s", str(self.s3_bucket_name))
        logger.debug("- s3_port: %s", str(self.s3_port))
        logger.debug("- s3_host: %s", str(self.s3_host))
        logger.debug("- sftp_hosts: %s", str(self.sftp_hosts))
        logger.debug("- sftp_username: %s", str(self.sftp_username))
        logger.debug("- sftp_private_key: %s", str(self.sftp_private_key))

    @property
    def archive(self):
        return os.path.join(self.dir_tmp_gen, 'puppet-config-environment.tar.xz')

    @property
    def conf_environment_gen(self):
        """Path where environment.conf is generated."""
        return os.path.join(self.dir_tmp_gen, self.conf_environment)

    @property
    def destination(self):
        return os.path.join(self.destination_root, self.environment, self.version)

conf = AppConf()            # global runtime configuration object
_sftp_host_directories = {} # global cache of remote directories

def parse_conf():
    """Parse configuration file and set runtime configuration accordingly.
       Here are defined default configuration file parameters."""
    defaults = StringIO(
      "[global]\n"
      "cluster = unknown\n"
      "environment = production\n"
      "version = latest\n"
      "mode = posix\n"
      "destination = /var/www/html/hpc-config\n"
      "[posix]\n"
      "file_mode = 644\n"
      "dir_mode = 755\n"
      "[s3]\n"
      "access_key = XXXXXXXX\n"
      "secret_key = YYYYYYYYYYYYYYYY\n"
      "bucket_name = system\n"
      "host = rgw.service.virtual\n"
      "port = 7480\n"
      "[sftp]\n"
      "hosts = localhost\n"
      "username = root\n"
      "private_key = /root/.ssh/id_rsa\n"
      "[paths]\n"
      "tmp = /tmp/puppet-config-push\n"
      "puppethpc = puppet-hpc\n"
      "privatedata = hpc-privatedata\n"
      "puppet_conf = ${privatedata}/puppet-config/${global:cluster}/puppet.conf\n"
      "hiera_conf = ${privatedata}/puppet-config/${global:cluster}/hiera.yaml\n"
      "facts_private = ${privatedata}/puppet-config/${global:cluster}/hpc-config-facts.yaml\n"
      "modules_generic = ${puppethpc}/puppet-config/cluster,${puppethpc}/puppet-config/modules,/usr/share/puppet/modules\n"
      "modules_private = ${privatedata}/puppet-config/${global:cluster}/modules\n"
      "manifests_generic = ${puppethpc}/puppet-config/manifests\n"
      "manifests_private = ${privatedata}/puppet-config/${global:cluster}/manifests\n"
      "hieradata_generic = ${puppethpc}/hieradata\n"
      "hieradata_private = ${privatedata}/hieradata\n"
      "files_private = ${privatedata}/files/${global:cluster}\n")
    parser = configparser.ConfigParser()
    parser._interpolation = configparser.ExtendedInterpolation()
    parser.readfp(defaults)
    parser.read(conf.conf_file)
    conf.cluster = parser.get('global', 'cluster')
    conf.environment = parser.get('global', 'environment')
    conf.version = parser.get('global', 'version')
    conf.mode = parser.get('global', 'mode')
    conf.destination_root = parser.get('global', 'destination')
    conf.dir_tmp = parser.get('paths', 'tmp')
    conf.conf_puppet = parser.get('paths', 'puppet_conf')
    conf.conf_hiera = parser.get('paths', 'hiera_conf')
    conf.facts_private = parser.get('paths', 'facts_private')
    conf.dir_modules_generic = parser.get('paths', 'modules_generic').split(',')
    conf.dir_modules_private = parser.get('paths', 'modules_private')
    conf.dir_manifests_generic = parser.get('paths', 'manifests_generic')
    conf.dir_manifests_private = parser.get('paths', 'manifests_private')
    conf.dir_hieradata_generic = parser.get('paths', 'hieradata_generic')
    conf.dir_hieradata_private = parser.get('paths', 'hieradata_private')
    conf.dir_files_private = parser.get('paths', 'files_private')
    conf.s3_access_key = parser.get('s3', 'access_key')
    conf.s3_secret_key = parser.get('s3', 'secret_key')
    conf.s3_bucket_name = parser.get('s3', 'bucket_name')
    conf.s3_host = parser.get('s3', 'host')
    conf.s3_port = int(parser.get('s3', 'port'))
    conf.sftp_hosts = parser.get('sftp', 'hosts').split(',')
    conf.sftp_username = parser.get('sftp', 'username')
    conf.sftp_private_key = parser.get('sftp', 'private_key')
    conf.posix_file_mode = int(parser.get('posix', 'file_mode'), 8)
    conf.posix_dir_mode = int(parser.get('posix', 'dir_mode'), 8)

def parse_args():
    """Parses CLI args, then set debug flag and configuration file path in
       runtime configuration accordingly, and returns the args."""
    parser = argparse.ArgumentParser()
    parser.add_argument('-d', '--debug',
                        help='Enable debug mode',
                        action='store_true')
    parser.add_argument('-c', '--conf',
                        help='Path to the configuration file',
                        nargs='?',
                        default='/etc/hpc-config/push.conf')
    parser.add_argument('-e', '--environment',
                        help='Name of the pushed environment',
                        nargs='?')
    parser.add_argument('-V', '--version',
                        help='Version of the pushed config',
                        nargs='?')
    parser.add_argument('--full-tmp-cleanup',
                        help='Full tmp dir cleanup.',
                        action='store_true')
    parser.add_argument('--enable-python-warnings',
                        help="Don't hide some Python warnings.",
                        action='store_true')
    args = parser.parse_args()

    if args.debug:
        conf.debug = True
    if args.conf:
        conf.conf_file = args.conf

    return args

def setup_warnings(enable=False):
    if not enable:
        warnings.filterwarnings("ignore", category=DeprecationWarning)
        warnings.filterwarnings("ignore", category=FutureWarning)

def setup_logger():

    if conf.debug is True:
        logger.setLevel(logging.DEBUG)
    else:
        logger.setLevel(logging.INFO)
    handler = logging.StreamHandler()
    formatter = logging.Formatter('%(levelname)s: %(message)s')
    handler.setFormatter(formatter)
    logger.addHandler(handler)

def override_conf(args):
    """Override configuration files parameters with args values."""
    if args.environment:
        conf.environment = args.environment
    if args.version:
        conf.version = args.version
    if args.full_tmp_cleanup:
        conf.full_tmp_cleanup = True

def mktmpd():
    """Make tmp generate dir and its parents."""
    if not os.path.isdir(conf.dir_tmp):
        os.makedirs(conf.dir_tmp)
    conf.dir_tmp_gen = tempfile.mkdtemp(dir=conf.dir_tmp)

def build_tarball():

    logger.info("creating archive %s", conf.archive)
    tar = tarfile.open(name=conf.archive, mode='w:xz', dereference=True)

    # generic modules
    seen_modules = []
    for modulesdir in conf.dir_modules_generic:

        if os.path.exists(modulesdir) and \
           os.path.isdir(modulesdir):

            # detect and raise error in case of module conflict
            new_modules = os.listdir(modulesdir)
            intersect = list(set(seen_modules) & set(new_modules))
            if len(intersect):
                logger.error("modules conflict in %s: %s", modulesdir, str(intersect))
                sys.exit(1)
            seen_modules += new_modules

            logger.debug("adding generic modules dir %s: %s", modulesdir, str(new_modules))
            tar.add(modulesdir, arcname=os.path.join(conf.environment, 'modules_generic'))
        else:
            logger.warning("Configured generic modules dir is missing: '%s'",
                           modulesdir)

    # private modules
    if os.path.exists(conf.dir_modules_private) and \
       os.path.isdir(conf.dir_modules_private):
        logger.debug("adding private modules dir %s", conf.dir_modules_private)
        tar.add(conf.dir_modules_private, arcname=os.path.join(conf.environment, 'modules_private'))
    else:
        logger.warning("Configured private modules dir is missing: '%s'",
                       conf.dir_modules_private)
    # generic manifests
    if os.path.exists(conf.dir_manifests_generic) and \
       os.path.isdir(conf.dir_manifests_generic):
        logger.debug("adding generic manifests dir %s", conf.dir_manifests_generic)
        tar.add(conf.dir_manifests_generic, arcname=os.path.join(conf.environment, 'manifests'))
    else:
        logger.warning("Configured generic manifests dir is missing: '%s'",
                       conf.dir_manifests_generic)
    # private manifests
    if os.path.exists(conf.dir_manifests_private) and \
       os.path.isdir(conf.dir_manifests_private):
        logger.debug("adding private manifests dir %s", conf.dir_manifests_private)
        tar.add(conf.dir_manifests_private, arcname=os.path.join(conf.environment, 'manifests'))
    else:
        logger.warning("Configured private manifests dir is missing: '%s'",
                       conf.dir_manifests_private)
    # generic hieradata
    if os.path.exists(conf.dir_hieradata_generic) and \
       os.path.isdir(conf.dir_hieradata_generic):
        logger.debug("adding generic hieradata dir %s", conf.dir_hieradata_generic)
        tar.add(conf.dir_hieradata_generic, arcname=os.path.join(conf.environment, 'hieradata', 'generic'))
    else:
        logger.warning("Configured generic hieradata dir is missing: '%s'",
                       conf.dir_hieradata_generic)
    # private hieradata
    if os.path.exists(conf.dir_hieradata_private) and \
       os.path.isdir(conf.dir_hieradata_private):
        logger.debug("adding private hieradata dir %s", conf.dir_hieradata_private)
        tar.add(conf.dir_hieradata_private, arcname=os.path.join(conf.environment, 'hieradata', 'private'))
    else:
        logger.warning("Configured private hieradata dir is missing: '%s'",
                       conf.dir_hieradata_private)

    logger.debug("adding environment conf")
    tar.add(conf.conf_environment_gen, arcname=os.path.join(conf.environment, conf.conf_environment))

    tar.close()

def gen_env_conf():

    with open(conf.conf_environment_gen, 'w+') as env_f:
        env_f.write("modulepath=modules_private:modules_generic\n")
        env_f.write("manifest=manifests/cluster.pp\n")

def _push_posix():

    logger.info("posix push: pushing data in %s", conf.destination)

    if not os.path.isdir(conf.destination):
        logger.debug("posix push: create destination dir %s", conf.destination)
        os.makedirs(conf.destination, exist_ok=True)

    logger.debug("posix push: copying tarball")
    shutil.copy(conf.archive, conf.destination)

    dir_files = os.path.join(conf.destination, 'files')
    if os.path.isdir(dir_files):
        logger.debug("posix push: removing push private files dir %s", dir_files)
        shutil.rmtree(dir_files)

    logger.debug("posix push: copying private files")
    # copytree() default copy_function is shutil.copy2() which does not manage
    # directories. When the file is a symlink, copytree() resolve the link and
    # directly call copy2() with the target of the link. It fails with errno 21
    # when the target is a directory. To avoid this bug, use an alternate copy
    # function conf_copy() to properly handle symlinks on directories.
    shutil.copytree(conf.dir_files_private, dir_files, copy_function=conf_copy)
    logger.debug("posix push: copying puppet conf")
    shutil.copy(conf.conf_puppet, conf.destination)
    logger.debug("posix push: copying hiera conf")
    shutil.copy(conf.conf_hiera, conf.destination)
    logger.debug("posix push: copying private facts")
    shutil.copy(conf.facts_private, conf.destination)

    # Set permissions
    for root, dirs, files in os.walk(conf.destination):
        for dir_name in dirs:
            os.chmod(os.path.join(root, dir_name), conf.posix_dir_mode)
        for file_name in files:
            os.chmod(os.path.join(root, file_name), conf.posix_file_mode)


def _list_upload_file_paths(source_path):
    logger.debug('Upload path: %s', source_path)
    # List files to upload
    upload_file_paths = []
    if os.path.isfile(source_path):
        relative_path = os.path.basename(source_path)
        upload_file_paths.append(relative_path)
    for (current_dir, subdirs, filenames) in os.walk(source_path, followlinks=True):
        for filename in filenames:
            absolute_path = os.path.join(current_dir, filename)
            # remove the source path and first /
            relative_path = absolute_path[(len(source_path)+1):]
            upload_file_paths.append(relative_path)
    return upload_file_paths

def _get_full_paths(source_path, destination_path, file_path):
    # Determine file paths
    if os.path.isfile(source_path):
        source_file_path = source_path
    else:
        source_file_path = os.path.join(source_path, file_path)
    logger.debug("Source file path is: %s (%s, %s)", source_file_path, source_path, file_path)
    dest_file_path = os.path.join(destination_path, file_path)
    logger.debug("Dest file path is: %s (%s, %s)", dest_file_path, destination_path, file_path)
    return source_file_path, dest_file_path


def _s3_upload_file(source_file_path,
                    bucket,
                    destination_file_path,
                    object_md5s=None):
    #max size in bytes before uploading in parts. between 1 and 5 GB recommended
    max_size = 20 * 1000 * 1000
    #size of parts when uploading in parts
    part_size = 6 * 1000 * 1000

    if object_md5s is None:
        oject_md5s = {}

    # Check if the file has changed
    if destination_file_path in object_md5s.keys():
        remote_md5 = object_md5s[destination_file_path]
    else:
        remote_key = bucket.get_key(destination_file_path)
        if remote_key is not None:
            remote_md5 = remote_key.etag[1:-1]
        else:
            remote_md5 = None
    local_md5 = hashlib.md5(open(source_file_path, 'rb').read()).hexdigest()
    if remote_md5 == local_md5:
        logger.debug("S3 upload: MD5 Match for file %s", source_file_path)
        return
    else:
        logger.debug("S3 upload: MD5 Mismatch for file %s (%s != %s)",
                     source_file_path,
                     remote_md5,
                     local_md5)


    # Determine upload method
    filesize = os.path.getsize(source_file_path)
    if filesize > max_size:
        logger.debug("S3 upload: multipart upload for %s", source_file_path)
        mp = bucket.initiate_multipart_upload(destination_file_path,
                                              policy='public-read')
        fp = open(source_file_path, 'rb')
        fp_num = 0
        while fp.tell() < filesize:
            fp_num += 1
            logger.debug("S3 upload: uploading part %i", fp_num)
            mp.upload_part_from_file(fp, fp_num, size=part_size)

        mp.complete_upload()
    else:
        logger.debug("S3 upload: singlepart upload for %s", source_file_path)
        k = boto.s3.key.Key(bucket)
        k.key = destination_file_path
        bytes_written = k.set_contents_from_filename(source_file_path,
                                                     policy='public-read')
        logger.debug("S3 upload: %d/%d bytes written for %s",
                     bytes_written, filesize, source_file_path)


def _s3_upload(source_path,
               bucket,
               destination_path,
               clean=True,
               object_md5s=None):
    upload_file_paths = _list_upload_file_paths(source_path)

    if object_md5s is None:
        object_md5s = {}
    pool = ThreadPool()
    results = {}

    dirs = []
    touched_objects = []


    # Upload the files
    for file_path in upload_file_paths:
        source_file_path, dest_file_path = _get_full_paths(
            source_path, destination_path, file_path)

        # Create remote directory if necessary
        dest_dir_name = os.path.dirname(dest_file_path) + "/"
        while dest_dir_name not in dirs \
                and dest_dir_name not in object_md5s.keys() \
                and bucket.get_key(dest_dir_name) is None:
            logger.debug("S3 upload: Creating directory %s", dest_dir_name)
            dest_dir = bucket.new_key(dest_dir_name)
            dest_dir.set_contents_from_string('', policy='public-read')
            dirs.append(dest_dir_name)
            touched_objects.append(dest_dir_name)
            # Continue with parent
            dest_dir_name = os.path.dirname(dest_dir_name[:-1]) + "/"

        results[dest_file_path] = pool.apply_async(
            _s3_upload_file,
            [source_file_path, bucket, dest_file_path, object_md5s]
        )
        touched_objects.append(dest_file_path)

    pool.close()
    finished = 0
    while finished < len(results):
        finished = 0
        for result in results.values():
            if result.ready():
                finished += 1
        logger.info("SFTP push: Transfered files %d/%d", finished, len(results))
        time.sleep(1)
    for dest_file_path, result in results.items():
        result.get()
    pool.join()
    return touched_objects

def _s3_list_md5(bucket, prefix):
    keys = bucket.list(prefix=prefix)
    md5s = {}
    for key in keys:
        md5s[key.name] = key.etag[1:-1]
    return md5s

def _s3_remove_old_objects(bucket, old_object_md5s, new_objects):
    """
        Remove all objects from the old list not in the new list.
    """
    key_names = []
    for object_name in old_object_md5s.keys():
        if object_name not in new_objects:
            if not object_name.endswith("/"):
                logger.debug("S3 push: Removing old file: %s", object_name)
                key_names.append(object_name)
                continue
            empty=True
            for file_name in new_objects:
                if file_name.startswith(object_name):
                    empty=False
            if empty:
                logger.debug("S3 push: Removing old dir: %s", object_name)
                key_names.append(object_name)
    logger.info("S3 push: %d files to remove.", len(key_names))
    result = bucket.delete_keys(key_names)
    error_count = len(result.errors)
    deleted_count = len(result.deleted)
    logger.debug("S3 push: %d deleted, %s errors", deleted_count, error_count)
    if error_count > 0:
        logger.error("S3 push: Failed to delete %d old keys", error_count)

def _s3_rmrf(bucket, prefix):
    keys = bucket.list(prefix=prefix)
    pool = ThreadPool()
    results = []
    counter = 1
    key_group = []
    for key in keys:
        key_group.append(key)
        counter += 1
        if counter == 1000:
            results.append(
                pool.apply_async(bucket.delete_keys, [key_group])
            )
            key_group=[]
            counter = 1
    results.append(
        pool.apply_async(bucket.delete_keys, [key_group])
    )
    pool.close()
    pool.join()
    deleted = 0
    errors = 0
    for result in results:
        report = result.get()
        deleted += len(report.deleted)
        errors += len(report.errors)
    logger.info("S3 push: Deleted %d keys, %d errors", deleted, errors)

def _push_s3():
    logger.info("S3 push: pushing data in bucket %s", conf.s3_bucket_name)

    conn = boto.connect_s3(
        aws_access_key_id=conf.s3_access_key,
        aws_secret_access_key=conf.s3_secret_key,
        host=conf.s3_host,
        port=conf.s3_port,
        is_secure=False,
        calling_format=boto.s3.connection.OrdinaryCallingFormat(),
    )
    bucket = conn.get_bucket(conf.s3_bucket_name)
    bucket.set_acl('public-read')

    logger.info("S3 push: get remote objects list")
    obj_md5s = _s3_list_md5(bucket, conf.destination)

    touched_objects = []

    logger.info("S3 push: copying tarball")
    lst = _s3_upload(conf.archive, bucket, conf.destination, object_md5s=obj_md5s)
    touched_objects = list(set(touched_objects + lst))


    logger.info("S3 push: copying private files")
    dir_files = os.path.join(conf.destination, 'files')
    lst = _s3_upload(conf.dir_files_private, bucket, dir_files, object_md5s=obj_md5s)
    touched_objects = list(set(touched_objects + lst))
    logger.info("S3 push: copying puppet conf")
    lst = _s3_upload(conf.conf_puppet, bucket, conf.destination, object_md5s=obj_md5s)
    touched_objects = list(set(touched_objects + lst))
    logger.info("S3 push: copying hiera conf")
    lst = _s3_upload(conf.conf_hiera, bucket, conf.destination, object_md5s=obj_md5s)
    touched_objects = list(set(touched_objects + lst))
    logger.info("S3 push: copying private facts")
    lst = _s3_upload(conf.facts_private, bucket, conf.destination, object_md5s=obj_md5s)
    touched_objects = list(set(touched_objects + lst))

    logger.info("S3 push: Removing old files")
    _s3_remove_old_objects(bucket, obj_md5s, touched_objects)

def _sftp_is_dir(sftp_client, path):
    if sftp_client not in _sftp_host_directories.keys():
        _sftp_host_directories[sftp_client] = []
    if path in _sftp_host_directories[sftp_client]:
        return True
    try:
        is_dir = stat.S_ISDIR(sftp_client.stat(path).st_mode)
    except FileNotFoundError:
        is_dir = False
    if is_dir:
        _sftp_host_directories[sftp_client].append(path)
    return is_dir

def _sftp_list_children(sftp_client, path):
    children_attr = sftp_client.listdir_attr(path)
    files = []
    directories = []
    for child_attr in children_attr:
        name = child_attr.filename
        if stat.S_ISDIR(child_attr.st_mode):
            directories.append(name)
            # cache result
            if sftp_client not in _sftp_host_directories.keys():
                _sftp_host_directories[sftp_client] = []
            full_path = os.path.join(path, name)
            if full_path not in _sftp_host_directories[sftp_client]:
                _sftp_host_directories[sftp_client].append(name)
        else:
            files.append(name)
    return directories, files

def _sftp_rmrf(sftp_client, path):
    _sftp_host_directories[sftp_client] = []
    if _sftp_is_dir(sftp_client, path):
        directories, files = _sftp_list_children(sftp_client, path)
        for directory in directories:
            directory_path = os.path.join(path, directory)
            _sftp_rmrf(sftp_client, directory_path)
        for filename in files:
            file_path = os.path.join(path, filename)
            sftp_client.remove(file_path)
        sftp_client.rmdir(path)
    else:
        try:
            sftp_client.remove(path)
            logger.debug("SFTP: Removing: %s" % path)
        except FileNotFoundError:
            logger.debug("SFTP: Try to remove a missing file: %s" % path)
    _sftp_host_directories[sftp_client] = []

def _sftp_mkdir(sftp_client, path, mode=0o755):
    if _sftp_is_dir(sftp_client, path):
        return
    else:
        parent = os.path.dirname(path[:-1])
        _sftp_mkdir(sftp_client, parent, mode)
    sftp_client.mkdir(path)
    sftp_client.chmod(path, mode)

def _sftp_upload(source_path, sftp_client, destination_path, clean=True):
    upload_file_paths = _list_upload_file_paths(source_path)

    for file_path in upload_file_paths:
        source_file_path, dest_file_path = _get_full_paths(
            source_path, destination_path, file_path)
        # Create remote directory if necessary
        dest_dir_name = os.path.dirname(dest_file_path)
        _sftp_mkdir(sftp_client, dest_dir_name)
        # Upload
        sftp_client.put(source_file_path, dest_file_path, confirm=False)
        # Set Perms
        sftp_client.chmod(dest_file_path, 0o644)

def _sftp_push_host(host, conf):

    key = paramiko.RSAKey.from_private_key_file(conf.sftp_private_key)
    username = conf.sftp_username

    try:
        transport = paramiko.Transport((host, 22))
        transport.connect(username=username, pkey=key)
    except socket.gaierror as e:
        logger.error("SFTP push: Failed to connect to host %s" % host)
        logger.info("SFTP push: Connection error: %s." % e)
        return
    except paramiko.ssh_exception.SSHException as e:
        logger.error("SFTP push: SSH failed to %s@%s" % (username, host))
        logger.info("SFTP push: SSH error: %s." % e)
        return
    sftp_client = paramiko.SFTPClient.from_transport(transport)

    logger.debug("SFTP push: Cleaning destination %s", conf.destination)
    _sftp_rmrf(sftp_client, conf.destination)

    logger.debug("SFTP push: copying tarball")
    _sftp_upload(conf.archive, sftp_client, conf.destination)

    logger.debug("SFTP push: copying private files")
    dir_files = os.path.join(conf.destination, 'files')
    _sftp_upload(conf.dir_files_private, sftp_client, dir_files)
    logger.debug("SFTP push: copying puppet conf")
    _sftp_upload(conf.conf_puppet, sftp_client, conf.destination)
    logger.debug("SFTP push: copying hiera conf")
    _sftp_upload(conf.conf_hiera, sftp_client, conf.destination)
    logger.debug("SFTP push: copying private facts")
    _sftp_upload(conf.facts_private, sftp_client, conf.destination)


def _push_sftp():
    logger.info("SFTP push: pushing data on hosts %s", conf.sftp_hosts)

    pool = Pool()
    results = {}
    for host in conf.sftp_hosts:
       results[host] = pool.apply_async(_sftp_push_host, [host, conf])
    pool.close()
    finished = 0
    while finished < len(results):
        finished = 0
        for result in results.values():
            if result.ready():
                finished += 1
        logger.info("SFTP push: Finished host %d/%d", finished, len(results))
        time.sleep(1)
    for host, result in results.items():
        result.get()
    pool.join()

def push():

    if conf.mode == 'posix':
        _push_posix()
    elif conf.mode == 's3':
        _push_s3()
    elif conf.mode == 'sftp':
        _push_sftp()
    else:
        logger.error("unsupported push mode %s", conf.mode)
        sys.exit(1)

def cleanup_run():
    """Remove the run tmp dir."""

    logger.debug("removing run tmp dir %s", conf.dir_tmp_gen)
    shutil.rmtree(conf.dir_tmp_gen)

def cleanup_full():
    """Remove the full app tmp dir."""
    if not os.path.isdir(conf.dir_tmp):
        logger.info("app tmp dir %s does not exists, nothing to remove.", conf.dir_tmp)
    else:
        logger.info("removing app tmp dir %s", conf.dir_tmp)
        shutil.rmtree(conf.dir_tmp)

def main():

    #
    # init
    #
    args = parse_args()
    setup_warnings(args.enable_python_warnings)
    setup_logger()
    parse_conf()
    override_conf(args)
    conf.dump()

    #
    # run
    #
    if conf.full_tmp_cleanup:
        cleanup_full()
    else:
        mktmpd()
        gen_env_conf()
        build_tarball()
        push()
        cleanup_run()

if __name__ == '__main__':
    main()
